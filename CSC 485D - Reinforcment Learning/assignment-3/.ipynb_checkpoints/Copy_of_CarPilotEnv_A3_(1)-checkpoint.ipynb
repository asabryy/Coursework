{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383b94c1",
   "metadata": {
    "id": "383b94c1"
   },
   "source": [
    "# CarMDP Environment, which is provided for your convenience. You should not change code of this environment. This Jupyter notebook is prepared by Kui Wu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b542a52b",
   "metadata": {
    "id": "b542a52b"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "\n",
    "from gym import Env\n",
    "\n",
    "class CarMDP(Env):\n",
    "    \"\"\"\n",
    "    Car MDP with simple stochastic dynamics.\n",
    "    The states are tuples with two elements:\n",
    "        - a position index (i, j)\n",
    "        - an integer from (0, 1, 2, 3) representing absolute orientation (see self.orientations in __init__ below)\n",
    "    For example, the state\n",
    "        s = (0, 1, 2)\n",
    "    represents the car in the cell with indices (0, 1) and oriented to face the South.\n",
    "    \"\"\"\n",
    "    def __init__(self, width, height, obstacles, goal_transition, initial_state, p_corr, base_reward=-0.01,\n",
    "                 collision_reward=-5., goal_reward=10., stagnation_penalty=-0.01):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid_map = np.ones((width, height))\n",
    "        for cell in obstacles:\n",
    "            self.grid_map[cell[0], cell[1]] = 0.\n",
    "        self.obstacles = obstacles\n",
    "        self.orientations = {0: 'North', 1: 'East', 2: 'South', 3: 'West'}\n",
    "        self.A = {0: 'Forward', 1: 'Left', 2: 'Right', 3: 'Brake'}\n",
    "        self.goal_transition = goal_transition  # Tuple containing start and end state for the 'goal transition'\n",
    "\n",
    "        self.p_corr = p_corr\n",
    "        self.p_err = (1. - p_corr)/2.\n",
    "\n",
    "        self.base_reward = base_reward\n",
    "        self.collision_reward = collision_reward\n",
    "        self.goal_reward = goal_reward\n",
    "        self.stagnation_penalty = stagnation_penalty\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "        \n",
    "        assert initial_state[0] >= 0 and initial_state[1] >= 0 and initial_state[0] < self.width and initial_state[1] < self.height and \\\n",
    "               initial_state[2] in self.orientations, \"ERROR: initial state {:} is not valid.\".format(init_state)\n",
    "        self.state_history = [initial_state]\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "        self.init_state=initial_state\n",
    "    \n",
    "       \n",
    "    def reset(self):\n",
    "        self.state_history = [self.init_state]\n",
    "        self.action_history = []\n",
    "        self.reward_history = []\n",
    "\n",
    "    def is_collision(self, state):\n",
    "        is_out_of_bounds = state[0] < 0 or state[0] >= self.width or state[1] < 0 or \\\n",
    "             state[1] >= self.height\n",
    "        return is_out_of_bounds or (state[0], state[1]) in self.obstacles\n",
    "\n",
    "    def transition_dynamics(self, state, action):\n",
    "        assert not self.is_collision(state), \"ERROR: can't take an action from a non-state.\"\n",
    "        delta = 1\n",
    "        orientation = state[2]\n",
    "\n",
    "        if self.orientations[orientation] == 'North':\n",
    "            left = (state[0] - delta, state[1] - delta)\n",
    "            forward = (state[0], state[1] - delta)\n",
    "            right = (state[0] + delta, state[1] - delta)\n",
    "        elif self.orientations[orientation] == 'West':\n",
    "            left = (state[0] - delta, state[1] + delta)\n",
    "            forward = (state[0] - delta, state[1])\n",
    "            right = (state[0] - delta, state[1] - delta)\n",
    "        elif self.orientations[orientation] == 'South':\n",
    "            left = (state[0] + delta, state[1] + delta)\n",
    "            forward = (state[0], state[1] + delta)\n",
    "            right = (state[0] - delta, state[1] + delta)\n",
    "        elif self.orientations[orientation] == 'East':\n",
    "            left = (state[0] + delta, state[1] - delta)\n",
    "            forward = (state[0] + delta, state[1])\n",
    "            right = (state[0] + delta, state[1] + delta)\n",
    "\n",
    "        # p gives categorical distribution over (state, left, forward, right)\n",
    "        if self.A[action] == 'Forward':\n",
    "            p = np.array([0., self.p_err, self.p_corr, self.p_err])\n",
    "        elif self.A[action] == 'Right':\n",
    "            p = np.array([0., 0., 2.*self.p_err, self.p_corr])\n",
    "        elif self.A[action] == 'Left':\n",
    "            p = np.array([0., self.p_corr, 2. * self.p_err, 0.])\n",
    "        elif self.A[action] == 'Brake':\n",
    "            p = np.array([self.p_corr, 0., 2. * self.p_err, 0.])\n",
    "\n",
    "        candidate_next_state_positions = (state, left, forward, right)\n",
    "        next_state_position = candidate_next_state_positions[categorical_sample_index(p)]\n",
    "\n",
    "        # Handle orientation dynamics (deterministic)\n",
    "        new_orientation = orientation\n",
    "        if self.A[action] == 'Right':\n",
    "            new_orientation = (orientation + 1) % 4\n",
    "        elif self.A[action] == 'Left':\n",
    "            new_orientation = (orientation - 1) % 4\n",
    "\n",
    "        return next_state_position[0], next_state_position[1], new_orientation\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert action in self.A, f\"ERROR: action {action} not permitted\"\n",
    "        terminal = False\n",
    "        current_state = self.state_history[-1] # -1 means the current element\n",
    "        next_state = self.transition_dynamics(current_state, action)\n",
    "        if self.is_collision(next_state):\n",
    "            reward = self.collision_reward\n",
    "            terminal = True\n",
    "        elif (current_state[0], current_state[1]) == self.goal_transition[0] and \\\n",
    "                (next_state[0], next_state[1]) == self.goal_transition[1]:\n",
    "            reward = self.goal_reward\n",
    "            terminal = True  # TODO: allow multiple laps like this?\n",
    "        elif current_state == next_state:\n",
    "            reward = self.stagnation_penalty\n",
    "            terminal = False\n",
    "        else:\n",
    "            reward = self.base_reward\n",
    "            terminal = False\n",
    "\n",
    "        self.state_history.append(next_state)\n",
    "        self.reward_history.append(reward)\n",
    "        self.action_history.append(action)\n",
    "\n",
    "        return next_state, reward, terminal, []\n",
    "\n",
    "    def render(self, title):\n",
    "        self._plot_history(title)\n",
    "    \n",
    "    def _plot_history(self, title):\n",
    "        \"\"\"\n",
    "        Plot the MDP's trajectory on the grid map.\n",
    "        :param title:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(self.grid_map.T, cmap='gray')\n",
    "        plt.grid()\n",
    "        x = np.zeros(len(self.state_history))\n",
    "        y = np.zeros(x.shape)\n",
    "        for idx in range(len(x)):\n",
    "            x[idx] = self.state_history[idx][0]\n",
    "            y[idx] = self.state_history[idx][1]\n",
    "            if self.state_history[idx][2] == 0:\n",
    "                plt.arrow(x[idx], y[idx], 0., -0.25, width=0.1)\n",
    "            elif self.state_history[idx][2] == 1:\n",
    "                plt.arrow(x[idx], y[idx], 0.25, 0., width=0.1)\n",
    "            elif self.state_history[idx][2] == 2:\n",
    "                plt.arrow(x[idx], y[idx], 0., 0.25, width=0.1)\n",
    "            else:\n",
    "                plt.arrow(x[idx], y[idx], -0.25, 0., width=0.1)\n",
    "\n",
    "        plt.plot(x, y, 'b-')  # Plot trajectory\n",
    "        plt.xlim([-0.5, self.width + 0.5])\n",
    "        plt.ylim([self.height + 0.5, -0.5])\n",
    "        plt.title(title)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()\n",
    "        return fig\n",
    "\n",
    "\n",
    "def categorical_sample_index(p: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Sample a categorical distribution.\n",
    "\n",
    "    :param p: a categorical distribution's probability mass function (i.e., p[idx] is the probability of this function\n",
    "              returning idx for an integer 0 <= idx < len(p)). I.e., np.sum(p) == 1 and p[idx] >= 0 for 0<=idx<len(p).\n",
    "    :return: index of a sample weighted by the categorical distribution described by p\n",
    "    \"\"\"\n",
    "    P = np.cumsum(p)\n",
    "    sample = np.random.rand()\n",
    "    return np.argmax(P > sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34981e",
   "metadata": {
    "id": "1f34981e"
   },
   "source": [
    "# Below is the skeleton code of your agent. Your solution should be filled here.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11d3c45",
   "metadata": {
    "id": "d11d3c45"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class ReinforcementLearningAgent:\n",
    "    \"\"\"\n",
    "    Your implementation of a reinforcement learning agent.\n",
    "    Feel free to add additional methods and attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, discount=0.9, epsilon=0.2, alpha=0.25):\n",
    "        ### STUDENT CODE GOES HERE\n",
    "        # Set any parameters\n",
    "        # You can add arguments to __init__, so log as they have default values (e.g., epsilon=0.1)\n",
    "        self.state_history = []\n",
    "        self.action_history = []\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.alpha = alpha\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.env_actions = lambda s : range(4) \n",
    "        pass\n",
    "    \n",
    "    def set_q(self, state, action, v):\n",
    "        self.Q[state][action] = v\n",
    "    \n",
    "    def get_q(self, state, action):\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "    def td_update(self, state, action, reward, next_state, next_action, terminal):\n",
    "        if not terminal:\n",
    "            td = reward + self.discount * self.get_q(next_state, next_action) - self.get_q(state, action)\n",
    "        else:\n",
    "            td = reward - self.get_q (state, action)\n",
    "        \n",
    "        v = self.get_q(state, action) + self.alpha * td\n",
    "        self.set_q(state, action, v)\n",
    "\n",
    "    def argmax_a(self, state):\n",
    "        acts = self.env_actions(state)\n",
    "        max_action = []\n",
    "        max_q = float('-inf')\n",
    "\n",
    "        for a in acts:\n",
    "            QV = self.get_q(state, a)\n",
    "            if QV > max_q:\n",
    "                max_action = [a]\n",
    "                max_q = QV\n",
    "            elif QV == max_q:\n",
    "                max_action.append(a)\n",
    "        \n",
    "        return np.random.choice(np.array(max_action))\n",
    "\n",
    "    def reset(self, init_state) -> int:\n",
    "        \"\"\"\n",
    "        Called at the start of each episode.\n",
    "\n",
    "        :param init_state:\n",
    "        :return: first action to take.\n",
    "        \"\"\"\n",
    "        self.state_history.append(init_state)\n",
    "        next_action = self.choose_action(init_state)\n",
    "        self.action_history.append(next_action)\n",
    "        ### STUDENT CODE GOES HERE\n",
    "        return next_action # Random policy (CHANGE THIS)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        acts = self.env_actions(state)\n",
    "\n",
    "        if len(acts) == 0:\n",
    "            return None\n",
    "        if np.random.random() < self.epsilon:\n",
    "            a = np.random.choice(acts)\n",
    "            return a\n",
    "        else:\n",
    "            a = self.argmax_a(state)\n",
    "            return a\n",
    "\n",
    "    def next_action(self, reward: float, state: int, terminal: bool) -> int:\n",
    "        \"\"\"\n",
    "        Called during each time step of a reinforcement learning episode\n",
    "\n",
    "        :param reward: reward resulting from the last time step's action\n",
    "        :param state: state resulting from the last time step's action\n",
    "        :param terminal: bool indicating whether state is a terminal state\n",
    "        :return: next action to take\n",
    "        \"\"\"\n",
    "        next_action = self.choose_action(state)\n",
    "        self.td_update(self.state_history[-1], self.action_history[-1], reward, state, next_action, terminal)\n",
    "        self.state_history.append(state)\n",
    "        self.action_history.append(next_action)\n",
    "        ### STUDENT CODE GOES HERE\n",
    "        # Produce the next action to take in an episode as a function of the observed reward and state\n",
    "        # You may find it useful to track past actions, states, and rewards\n",
    "        # Additionally, algorithms that learn during an episode (e.g., temporal difference) may find use for this method\n",
    "        return next_action  # Random policy (CHANGE THIS)\n",
    "\n",
    "    def finish_episode(self):\n",
    "        \"\"\"\n",
    "        Called at the end of each episode.\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "        ### STUDENT CODE GOES HERE\n",
    "        # Algorithms that learn from an entire episode (e.g., Monte Carlo) may find a use for this method\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c0d7f",
   "metadata": {
    "id": "302c0d7f"
   },
   "source": [
    "# Below is the sample test code. In the final print out you need to print out the correct policy name (It is random so far). Note that since this is a model-free solution, we will use a different test environment (i.e., the locations and the sizes of barriers are different) to test your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9801a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "id": "8de9801a",
    "outputId": "1eb87996-4d5c-42c9-8ca4-5dbc959dd98c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing with: epsilon=0.01 and alpha=0.25\n"
     ]
    }
   ],
   "source": [
    "def test_rl_algorithm(rl_agent, car_mdp, initial_state, n_episodes=10000, n_plot=np.inf):\n",
    "    \"\"\"\n",
    "    Code that will be used to test your implementation of ReinforcementLearningAgent.\n",
    "    As you can see, you are responsible for implementing three methods in ReinforcementLearningAgent:\n",
    "        - reset (called at the start of every episode)\n",
    "        - next_action (called at every time step of an episode)\n",
    "        - finish_episode (called at the end of each episode)\n",
    "\n",
    "    :param rl_agent: an instance of your ReinforcementLearningAgent class\n",
    "    :param car_mdp: an instance of CarMDP\n",
    "    :param init_state: the initial state\n",
    "    :param n_episodes: number of episodes to use for this test\n",
    "    :param n_plot: display a plot every n_plot episodes\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    for episode in range(n_episodes):\n",
    "        G = 0.  # Keep track of the returns for this episode (discount factor gamma=1)\n",
    "        # Re-initialize the MDP and the RL agent\n",
    "        car_mdp.reset();\n",
    "        action = rl_agent.reset(initial_state)\n",
    "        terminal = False\n",
    "        while not terminal:  # Loop until a terminal state is reached\n",
    "            next_state, reward, terminal, [] = car_mdp.step(action)\n",
    "            G += reward\n",
    "            action = rl_agent.next_action(reward, next_state, terminal)\n",
    "        rl_agent.finish_episode()\n",
    "        returns += [G]\n",
    "\n",
    "        # Plot the trajectory every n_plot episodes\n",
    "        if episode % n_plot == 0 and episode > 0:\n",
    "            car_mdp.render('State History ' + str(episode + 1))\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Size of the CarMDP map (any cell outside of this rectangle is a terminal state)\n",
    "    width = 9\n",
    "    height = 6\n",
    "    initial_state = (0, 0, 2)  # Top left corner (0, 0), facing \"Down\" (2)\n",
    "    obstacles = [(2, 2), (2, 3), (3, 2), (3, 3),  # Cells filled with obstacles are terminal states\n",
    "                 (5, 2), (5, 3), (6, 2), (6, 3),\n",
    "                 (1, 1), (1, 2)]\n",
    "    goal_transition = ((1, 0), (0, 0))  # Transitioning from cell (1, 0) to cell (0, 0) terminates and gives a reward\n",
    "    p_corr = 0.95  # Probability of actions working as intended\n",
    "    \n",
    "  # Create environment\n",
    "    car_mdp = CarMDP(width, height, obstacles, goal_transition, initial_state, p_corr=p_corr)\n",
    "  \n",
    " # Create RL agent. # You must complete this class in your solution, it is random policy right now\n",
    " \n",
    "    rl_agent = ReinforcementLearningAgent()  \n",
    "    \n",
    "   \n",
    "    #student_returns = test_rl_algorithm(rl_agent, car_mdp, initial_state, n_episodes=50000, n_plot=4999)\n",
    "\n",
    "    # Example plot. You need to change it according to the assignment requirements. \n",
    "    n_runs = 20\n",
    "    n_episodes = 10000\n",
    "    epsilon = [0.01, 0.05, 0.1]\n",
    "    alpha = [0.25, 0.5, 0.75]\n",
    "    discount = [0.9]\n",
    "    tests = []\n",
    "    avg_returns = []\n",
    "    test = []\n",
    "    final_graph = []\n",
    "    plt.grid()\n",
    "    plt.figure()\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Return')\n",
    "    for e in epsilon:\n",
    "        for a in alpha:\n",
    "            for d in discount:\n",
    "                returns = np.zeros((n_runs, n_episodes))\n",
    "                print(\"testing with: epsilon={:.2f} and alpha={:.2f}\".format(e, a))\n",
    "                for run in range(n_runs):\n",
    "                    rl_agentnew = ReinforcementLearningAgent(d, e, a)\n",
    "                    returns[run, :] = test_rl_algorithm(rl_agentnew, car_mdp, initial_state, n_episodes=n_episodes)\n",
    "                test = \"e,a,d = {},{},{}\".format(e, a, d)\n",
    "                tests.append(test)\n",
    "                mean_re = np.mean(np.mean(returns, axis=0))\n",
    "                print(mean_re.shape)\n",
    "                avg_returns.append(mean_re)\n",
    "\n",
    "                # Plot one curve like this for each parameter setting - the template code for ReinforcementLearningAgent just\n",
    "                # returns a random action, so this example curve will just be noise. When your method is working, the mean return\n",
    "                # should increase as the number of episodes increases. Feel free to change the rolling average width\n",
    "                rolling_average_width = 100\n",
    "                # Compute the mean (over n_runs) for each episode\n",
    "                mean_return = np.mean(returns, axis=0)\n",
    "                # Compute the rolling average (over episodes) to smooth out the curve\n",
    "                rolling_average_mean_return = np.convolve(mean_return, np.ones(rolling_average_width), 'valid')/rolling_average_width\n",
    "                final_graph.append(rolling_average_mean_return)\n",
    "                plt.plot(rolling_average_mean_return, label=r\"$\\epsilon$={:.2f}/$\\alpha$={:.2f}$\".format(e, a))  # Plot the smoothed average return for each episode over n_runs\n",
    "          \n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d8883",
   "metadata": {
    "id": "265d8883"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of CarPilotEnv-A3 (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
