{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29453210",
   "metadata": {},
   "source": [
    "# Brief Introduction of OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00182e5",
   "metadata": {},
   "source": [
    "OpenAI Gym leverages the concept of Object-Oriented Programming and defines the \"bare\" Env class as its core. In order to create your own Gym environment (e.g., myEnv), you need to import the \"bare\" Env class and override its funcations, whenever necessary. In other words, your myEnv is a child class of Gym Env. \n",
    "\n",
    "The best way to learn how to build your own Gym env is to study the sample environments already implemented in Gym, e.g., CartPole. Other useful classes defined in Gym can be found in the source code of Gym (in the directories of \"spaces\", \"envs\", and \"utils\". \n",
    "\n",
    "I would like to suggest that you know the details of the basic functions defined in the Gym Env class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" core.py in Gym source code \"\"\"\n",
    "\n",
    "from abc import abstractmethod\n",
    "\n",
    "import gym\n",
    "from gym import error\n",
    "from gym.utils import closer\n",
    "\n",
    "\n",
    "class Env(object):\n",
    "    \"\"\"The main OpenAI Gym class. It encapsulates an environment with\n",
    "    arbitrary behind-the-scenes dynamics. An environment can be\n",
    "    partially or fully observed.\n",
    "\n",
    "    The main API methods that users of this class need to know are:\n",
    "\n",
    "        step\n",
    "        reset\n",
    "        render\n",
    "        close\n",
    "        seed\n",
    "\n",
    "    And set the following attributes:\n",
    "\n",
    "        action_space: The Space object corresponding to valid actions\n",
    "        observation_space: The Space object corresponding to valid observations\n",
    "        reward_range: A tuple corresponding to the min and max possible rewards\n",
    "\n",
    "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
    "\n",
    "    The methods are accessed publicly as \"step\", \"reset\", etc...\n",
    "    \"\"\"\n",
    "\n",
    "    # Set this in SOME subclasses\n",
    "    metadata = {\"render.modes\": []}\n",
    "    reward_range = (-float(\"inf\"), float(\"inf\"))\n",
    "    spec = None\n",
    "\n",
    "    # Set these in ALL subclasses\n",
    "    action_space = None\n",
    "    observation_space = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, action):\n",
    "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
    "        episode is reached, you are responsible for calling `reset()`\n",
    "        to reset this environment's state.\n",
    "\n",
    "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
    "\n",
    "        Args:\n",
    "            action (object): an action provided by the agent\n",
    "\n",
    "        Returns:\n",
    "            observation (object): agent's observation of the current environment\n",
    "            reward (float) : amount of reward returned after previous action\n",
    "            done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to an initial state and returns an initial\n",
    "        observation.\n",
    "\n",
    "        Note that this function should not reset the environment's random\n",
    "        number generator(s); random variables in the environment's state should\n",
    "        be sampled independently between multiple calls to `reset()`. In other\n",
    "        words, each call of `reset()` should yield an environment suitable for\n",
    "        a new episode, independent of previous episodes.\n",
    "\n",
    "        Returns:\n",
    "            observation (object): the initial observation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Renders the environment.\n",
    "\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "\n",
    "        Example:\n",
    "\n",
    "        class MyEnv(Env):\n",
    "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "            def render(self, mode='human'):\n",
    "                if mode == 'rgb_array':\n",
    "                    return np.array(...) # return RGB frame suitable for video\n",
    "                elif mode == 'human':\n",
    "                    ... # pop up a window and render\n",
    "                else:\n",
    "                    super(MyEnv, self).render(mode=mode) # just raise an exception\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        \"\"\"Completely unwrap this env.\n",
    "\n",
    "        Returns:\n",
    "            gym.Env: The base non-wrapped gym.Env instance\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.spec is None:\n",
    "            return \"<{} instance>\".format(type(self).__name__)\n",
    "        else:\n",
    "            return \"<{}<{}>>\".format(type(self).__name__, self.spec.id)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Support with-statement for the environment.\"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        \"\"\"Support with-statement for the environment.\"\"\"\n",
    "        self.close()\n",
    "        # propagate exception\n",
    "        return False\n",
    "\n",
    "\n",
    "class GoalEnv(Env):\n",
    "    \"\"\"A goal-based environment. It functions just as any regular OpenAI Gym environment but it\n",
    "    imposes a required structure on the observation_space. More concretely, the observation\n",
    "    space is required to contain at least three elements, namely `observation`, `desired_goal`, and\n",
    "    `achieved_goal`. Here, `desired_goal` specifies the goal that the agent should attempt to achieve.\n",
    "    `achieved_goal` is the goal that it currently achieved instead. `observation` contains the\n",
    "    actual observations of the environment as per usual.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        # Enforce that each GoalEnv uses a Goal-compatible observation space.\n",
    "        if not isinstance(self.observation_space, gym.spaces.Dict):\n",
    "            raise error.Error(\n",
    "                \"GoalEnv requires an observation space of type gym.spaces.Dict\"\n",
    "            )\n",
    "        for key in [\"observation\", \"achieved_goal\", \"desired_goal\"]:\n",
    "            if key not in self.observation_space.spaces:\n",
    "                raise error.Error(\n",
    "                    'GoalEnv requires the \"{}\" key to be part of the observation dictionary.'.format(\n",
    "                        key\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
    "        \"\"\"Compute the step reward. This externalizes the reward function and makes\n",
    "        it dependent on a desired goal and the one that was achieved. If you wish to include\n",
    "        additional rewards that are independent of the goal, you can include the necessary values\n",
    "        to derive it in 'info' and compute it accordingly.\n",
    "\n",
    "        Args:\n",
    "            achieved_goal (object): the goal that was achieved during execution\n",
    "            desired_goal (object): the desired goal that we asked the agent to attempt to achieve\n",
    "            info (dict): an info dictionary with additional information\n",
    "\n",
    "        Returns:\n",
    "            float: The reward that corresponds to the provided achieved goal w.r.t. to the desired\n",
    "            goal. Note that the following should always hold true:\n",
    "\n",
    "                ob, reward, done, info = env.step()\n",
    "                assert reward == env.compute_reward(ob['achieved_goal'], ob['goal'], info)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Wrapper(Env):\n",
    "    \"\"\"Wraps the environment to allow a modular transformation.\n",
    "\n",
    "    This class is the base class for all wrappers. The subclass could override\n",
    "    some methods to change the behavior of the original environment without touching the\n",
    "    original code.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Don't forget to call ``super().__init__(env)`` if the subclass overrides :meth:`__init__`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.reward_range = self.env.reward_range\n",
    "        self.metadata = self.env.metadata\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name.startswith(\"_\"):\n",
    "            raise AttributeError(\n",
    "                \"attempted to get missing private attribute '{}'\".format(name)\n",
    "            )\n",
    "        return getattr(self.env, name)\n",
    "\n",
    "    @property\n",
    "    def spec(self):\n",
    "        return self.env.spec\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls):\n",
    "        return cls.__name__\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def render(self, mode=\"human\", **kwargs):\n",
    "        return self.env.render(mode, **kwargs)\n",
    "\n",
    "    def close(self):\n",
    "        return self.env.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        return self.env.seed(seed)\n",
    "\n",
    "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
    "        return self.env.compute_reward(achieved_goal, desired_goal, info)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<{}{}>\".format(type(self).__name__, self.env)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self.env.unwrapped\n",
    "\n",
    "\n",
    "class ObservationWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        observation = self.env.reset(**kwargs)\n",
    "        return self.observation(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        return self.observation(observation), reward, done, info\n",
    "\n",
    "    @abstractmethod\n",
    "    def observation(self, observation):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RewardWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = self.env.step(action)\n",
    "        return observation, self.reward(reward), done, info\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self, reward):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ActionWrapper(Wrapper):\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(self.action(action))\n",
    "\n",
    "    @abstractmethod\n",
    "    def action(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def reverse_action(self, action):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf60b19",
   "metadata": {},
   "source": [
    "Using CartPoleEnv as an example, we know that we at least need to override the following function:\n",
    "\n",
    "* `__init__(self)`\n",
    "* `step(self, action)` # Be careful about the return values of this function \n",
    "* `reset(self)` # This function is needed every time when a new episode starts\n",
    "* `seed(self, seed=None)` #This is optional but needed if our code needs to generate random numbers (most likely). \n",
    "\n",
    "\n",
    "We may also need to override `render` (and other related functions) if the environment includes GUI.\n",
    "\n",
    "\n",
    "In addition, we need to define our own state and other housekeeping variables in our own environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along\n",
    "        a frictionless track. The pendulum starts upright, and the goal is to\n",
    "        prevent it from falling over by increasing and reducing the cart's\n",
    "        velocity.\n",
    "\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem\n",
    "        described by Barto, Sutton, and Anderson\n",
    "\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "\n",
    "        Note: The amount the velocity that is reduced or increased is not\n",
    "        fixed; it depends on the angle the pole is pointing. This is because\n",
    "        the center of gravity of the pole increases the amount of energy needed\n",
    "        to move the cart underneath it\n",
    "\n",
    "    Reward:\n",
    "        Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "\n",
    "    Episode Termination:\n",
    "        Pole Angle is more than 12 degrees.\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of\n",
    "        the display).\n",
    "        Episode length is greater than 200.\n",
    "        Solved Requirements:\n",
    "        Considered solved when the average return is greater than or equal to\n",
    "        195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"], \"video.frames_per_second\": 50}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5  # actually half the pole's length\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = \"euler\"\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
    "        # is still within bounds.\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                self.theta_threshold_radians * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = \"%r (%s) invalid\" % (action, type(action))\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "\n",
    "        # For the interested reader:\n",
    "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "        temp = (\n",
    "            force + self.polemass_length * theta_dot ** 2 * sintheta\n",
    "        ) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else:  # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "            or theta < -self.theta_threshold_radians\n",
    "            or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\n",
    "                    \"You are calling 'step()' even though this \"\n",
    "                    \"environment has already returned done = True. You \"\n",
    "                    \"should always call 'reset()' once you receive 'done = \"\n",
    "                    \"True' -- any further steps are undefined behavior.\"\n",
    "                )\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        carty = 100  # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "            axleoffset = cartheight / 4.0\n",
    "            cart = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l, r, t, b = (\n",
    "                -polewidth / 2,\n",
    "                polewidth / 2,\n",
    "                polelen - polewidth / 2,\n",
    "                -polewidth / 2,\n",
    "            )\n",
    "            pole = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            pole.set_color(0.8, 0.6, 0.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth / 2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(0.5, 0.5, 0.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0, carty), (screen_width, carty))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "        pole.v = [(l, b), (l, t), (r, t), (r, b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0] * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec004f51",
   "metadata": {},
   "source": [
    "After you implement the CartPoleEnv environment as above, you can use the following template test code to test this environment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Template to test an environment\"\"\"\n",
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v0') #Note that CartPole-v0 is the name that you use to register for CartPoleEnv in Gym.    \n",
    "env.reset()\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action # You can print out the returns of the step function to find out the feedback of the environment. \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9f4a33",
   "metadata": {},
   "source": [
    "That is it! The CartPole environment is ready. The rest is to build RL agent that can learn and generate the optimal policy based on the interaction with the environment. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
